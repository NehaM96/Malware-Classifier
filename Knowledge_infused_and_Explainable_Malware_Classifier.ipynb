{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTlrYHaaTBvvd3N/xmYIWr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NehaM96/Malware-Classifier/blob/main/Knowledge_infused_and_Explainable_Malware_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonlines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV8MEVrGOwss",
        "outputId": "4e3ad999-962c-4608-c19b-6ad316cd705e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonpath_ng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgrpyC-jO3bH",
        "outputId": "5f093ac6-f233-4a53-c54c-ce03bd1bffc3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonpath_ng\n",
            "  Downloading jsonpath_ng-1.6.1-py3-none-any.whl (29 kB)\n",
            "Collecting ply (from jsonpath_ng)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ply, jsonpath_ng\n",
            "Successfully installed jsonpath_ng-1.6.1 ply-3.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9rWZenTzLLgy"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "from jsonpath_ng import parse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_columns = ['sha256', 'md5', 'histogram', 'byteentropy', 'numstrings', 'avlength',\n",
        "               'printabledist', 'printables', 'string_entropy', 'paths', 'urls',\n",
        "               'registry', 'MZ', 'size', 'vsize', 'has_debug', 'exports', 'imports', 'has_relocations',\n",
        "               'has_resources', 'has_signature', 'has_tls', 'symbols',\n",
        "               'header_coff_machine', 'header_coff_characteristics', 'header_optional_subsystem',\n",
        "               'header_optional_dll_characteristics', 'header_optional_magic', 'header_optional_major_image_version',\n",
        "               'header_optional_minor_image_version', 'header_optional_major_linker_version',\n",
        "               'header_optional_minor_linker_version', 'header_optional_major_operating_system_version',\n",
        "               'header_optional_minor_operating_system_version', 'header_optional_major_subsystem_version',\n",
        "               'header_optional_minor_subsystem_version', 'header_optional_sizeof_code',\n",
        "               'header_optional_sizeof_headers', 'header_optional_sizeof_heap_commit', 'section', 'imports',\n",
        "                    'exports', 'datadirectories', 'label', 'avclass']\n",
        "\n",
        "jsonpath_columns = ['sha256', 'md5', 'histogram', 'byteentropy', 'strings/numstrings',\n",
        "                    'strings/avlength','strings/printabledist', 'strings/printables', 'strings/entropy',\n",
        "                    'strings/paths', 'strings/urls',\n",
        "               'strings/registry', 'strings/MZ', 'general/size', 'general/vsize', 'general/has_debug',\n",
        "                    'general/exports', 'general/imports', 'general/has_relocations',\n",
        "               'general/has_resources', 'general/has_signature', 'general/has_tls', 'general/symbols',\n",
        "               'header/coff/machine', 'header/coff/characteristics', 'header/optional/subsystem',\n",
        "               'header/optional/dll_characteristics', 'header/optional/magic', 'header/optional/major_image_version',\n",
        "               'header/optional/minor_image_version', 'header/optional/major_linker_version',\n",
        "               'header/optional/minor_linker_version', 'header/optional/major_operating_system_version',\n",
        "               'header/optional/minor_operating_system_version', 'header/optional/major_subsystem_version',\n",
        "               'header/optional/minor_subsystem_version', 'header/optional/sizeof_code',\n",
        "               'header/optional/sizeof_headers', 'header/optional/sizeof_heap_commit', 'section', 'imports',\n",
        "                    'exports', 'datadirectories', 'label', 'avclass']\n",
        "print(len(jsonpath_columns))\n",
        "print(len(pandas_columns))\n",
        "non_numerical_columns = ['sha256', 'md5', 'histogram', 'header_coff_machine', 'header_coff_characteristics',\n",
        "                         'header_optional_subsystem', 'printabledist', 'byteentropy',\n",
        "                         'header_optional_dll_characteristics', 'header_optional_magic']\n",
        "\n",
        "DATASET_CSV = 'dataset4.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7cpvgZHON6n",
        "outputId": "65991f52-dd09-4284-c9bf-2abd1e222307"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n",
            "45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_jsonpath_value(json_data, col):\n",
        "    col = '.'.join(col.split('/'))\n",
        "    expr = parse(col)\n",
        "    matches = expr.find(json_data)\n",
        "    return matches[0].value if matches else None\n",
        "\n",
        "\n",
        "def write_row_into_csv(filename, row):\n",
        "    with open(filename, 'a', newline='', encoding='utf-8') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "\n",
        "def process_line(line):\n",
        "    row = []\n",
        "    for col in jsonpath_columns:\n",
        "        if '/' in col:\n",
        "            row.append(get_jsonpath_value(line, col))\n",
        "        else:\n",
        "            row.append(line[col])\n",
        "    write_row_into_csv(DATASET_CSV, row)\n",
        "\n",
        "\n",
        "def prepare_dataset_parallel(filename):\n",
        "    with jsonlines.open('datasets/ember2018/' + filename) as f:\n",
        "        lines = list(f.iter())\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=40) as executor:  # Adjust the max_workers parameter as needed\n",
        "        futures = []\n",
        "        for line in tqdm(lines):\n",
        "            if line['label'] != -1:\n",
        "                future = executor.submit(process_line, line)\n",
        "                futures.append(future)\n",
        "        for future in futures:\n",
        "            future.result()\n",
        "\n",
        "def add_new_columns():\n",
        "    df = pd.read_csv(DATASET_CSV).drop(columns=['Unnamed: 0'])\n",
        "    files = ['train_features_1.jsonl']\n",
        "    datadirectories = []\n",
        "    for filename in files:\n",
        "        with jsonlines.open('datasets/ember2018/' + filename) as f:\n",
        "            lines = list(f.iter())\n",
        "        for line in tqdm(lines):\n",
        "            if line['label'] != -1:\n",
        "                datadirectories.append(line['datadirectories'])\n",
        "    df['datadirectories'] = datadirectories\n",
        "    df.to_csv(DATASET_CSV)\n",
        "    print(df.columns)"
      ],
      "metadata": {
        "id": "LQsxjNyPOJ0J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = ['train_features_1.jsonl']\n",
        "for filename in files:\n",
        "  prepare_dataset_parallel(filename)"
      ],
      "metadata": {
        "id": "LzLB25INOl16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from py2neo import Graph, Node, Relationship\n",
        "from neo4j import GraphDatabase\n",
        "from sklearn.metrics import accuracy_score, auc, roc_curve, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import csv\n",
        "import sys\n",
        "import csv\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "model_filename = 'lightgbm_model.txt'\n",
        "cutoff = 18442\n",
        "df = pd.read_csv(DATASET_CSV, encoding='utf8', nrows=cutoff)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "324sEdPK9Abq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = Graph(\"bolt://localhost:7687\", name='mitreember', auth=(\"neo4j\", \"12345678\"))"
      ],
      "metadata": {
        "id": "i2Qz5WkR6KA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_label(txt):\n",
        "\n",
        "\tif txt.startswith('intrusion-set'):\n",
        "\t\treturn 'Group'\n",
        "\tif txt.startswith('malware'):\n",
        "\t\treturn 'Software'\n",
        "\tif txt.startswith('tool'):\n",
        "\t\treturn 'Tool'\n",
        "\tif txt.startswith('attack-pattern'):\n",
        "\t\treturn 'Technique'\n",
        "\tif txt.startswith('course-of-action'):\n",
        "\t\treturn 'Technique'\n",
        "\treturn 'Unknown'\n",
        "\n",
        "\n",
        "def build_objects(obj,key):\n",
        "\tlabel = build_label(obj['type'])\n",
        "\tprops = {'name': obj['name'], 'id': obj['id'], 'type': obj['type']}\n",
        "\tif obj.get('description'):\t\tprops['description'] = obj['description']\n",
        "\tnode_main = Node(label, **props)\n",
        "\tgraph.merge(node_main,label,'name')\n",
        "\tprint('%s: \"%s\"' % (label,obj['name']),end='')\n",
        "\n",
        "\n",
        "\tif obj.get('aliases'):\n",
        "\t\taliases = obj['aliases']\n",
        "\telif obj.get('x_mitre_aliases'):\n",
        "\t\taliases = obj['x_mitre_aliases']\n",
        "\telse:\n",
        "\t\taliases = None\n",
        "\tif aliases:\n",
        "\t\tfor alias in aliases:\n",
        "\t\t\tif alias != obj['name']:\n",
        "\t\t\t\tnode_alias = Node('Alias', name=alias, type=obj['type'])\n",
        "\t\t\t\trelation = Relationship.type('alias')\n",
        "\t\t\t\tgraph.merge(relation(node_main,node_alias),label,'name')\n",
        "\t\t\t\tprint(' -[alias]-> \"%s\"' % (alias),end='')\n",
        "\n",
        "\n",
        "\n",
        "def build_relations(obj):\n",
        "\n",
        "\tif not gnames.get(obj['source_ref']): return\n",
        "\tif not gnames.get(obj['target_ref']): return\n",
        "\n",
        "\tm = NodeMatcher(graph)\n",
        "\n",
        "\tsource = m.match( build_label(obj['source_ref']), name=gnames[obj['source_ref']] ).first()\n",
        "\ttarget = m.match( build_label(obj['target_ref']), name=gnames[obj['target_ref']] ).first()\n",
        "\n",
        "\n",
        "\trelation = Relationship.type( obj['relationship_type'] )\n",
        "\n",
        "\tgraph.merge(relation(source,target),build_label(obj['source_ref']),'name')\n",
        "\tprint('Relation: \"%s\" -[%s]-> \"%s\"' % (gnames[obj['source_ref']],obj['relationship_type'],gnames[obj['target_ref']]) )\n",
        "\n"
      ],
      "metadata": {
        "id": "nC5Ur49P8Eef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "from py2neo import Graph, Node, Relationship, NodeMatcher, cypher\n",
        "\n",
        "\n",
        "json_file = 'enterprise-attack.json'\n",
        "\n",
        "try:\n",
        "\twith open(json_file) as fh:\n",
        "\t\tdata = json.load(fh)\n",
        "\tfh.close()\n",
        "except Exception as e:\n",
        "\tsys.stderr.write( '[ERROR] reading configuration file %s\\n' % json_file )\n",
        "\tsys.stderr.write( '[ERROR] %s\\n' % str(e) )\n",
        "\tsys.exit(1)\n",
        "\n",
        "gnames = {}\n",
        "\n",
        "\n",
        "for obj in data['objects']:\n",
        "\n",
        "\n",
        "\tif obj['type'] =='intrusion-set':\n",
        "\t\tgnames[obj['id']] = obj['name']\n",
        "\t\tbuild_objects(obj,'intrusion-set')\n",
        "\n",
        "\n",
        "\tif obj['type']=='malware':\n",
        "\t\tgnames[ obj['id'] ] = obj['name']\n",
        "\t\tbuild_objects(obj,'malware')\n",
        "\n",
        "\n",
        "\tif obj['type']=='tool':\n",
        "\t\tgnames[ obj['id'] ] = obj['name']\n",
        "\t\tbuild_objects(obj,'tool')\n",
        "\n",
        "\n",
        "\tif obj['type']=='attack-pattern':\n",
        "\t\tgnames[ obj['id'] ] = obj['name']\n",
        "\t\tbuild_objects(obj,'attack-pattern')\n",
        "\n",
        "  if obj['type']=='course-of-action':\n",
        "    gnames[ obj['id'] ] = obj['name']\n",
        "    build_objects(obj,'course-of-action')\n",
        "\n",
        "for obj in data['objects']:\n",
        "\tif obj['type'] =='relationship':\n",
        "\t\tbuild_relations(obj)\n"
      ],
      "metadata": {
        "id": "7huFxJ8N7lvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_node_if_not_exists(label, properties):\n",
        "    # Check if a node with the same properties exists\n",
        "    existing_node = graph.nodes.match(label, **properties).first()\n",
        "\n",
        "    if existing_node:\n",
        "        return existing_node\n",
        "    else:\n",
        "        new_node = Node(label, **properties)\n",
        "        graph.create(new_node)\n",
        "        return new_node\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_relationship(start_node, relationship_type, end_node):\n",
        "    # Check if a relationship already exists\n",
        "    existing_rel = graph.match_one(nodes=[start_node, end_node], r_type=relationship_type)\n",
        "\n",
        "    if not existing_rel:\n",
        "        graph.create(Relationship(start_node, relationship_type, end_node))\n",
        "\n",
        "\n",
        "\n",
        "header = False\n",
        "with open('dataset4.csv', newline='', encoding='utf8') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in tqdm(reader):\n",
        "        if not header:\n",
        "            header = True\n",
        "            continue\n",
        "        try:\n",
        "            section = json.loads(row[40].replace(\"'\", '\"'))\n",
        "        except:\n",
        "            section = {\"entry\": \"UNK\", \"sections\": []}\n",
        "        datapoint_node = create_node_if_not_exists(\"Datapoint\", {\"index\": row[0],\n",
        "                                                                 \"sha256\": row[1]})\n",
        "        entry = create_node_if_not_exists(\"EntrySection\", {\"name\": section['entry']})\n",
        "        create_relationship(datapoint_node, \"HAS_ENTRY_SECTION\", entry)\n",
        "        for section in section[\"sections\"]:\n",
        "            section_node = create_node_if_not_exists(\"Section\", {\n",
        "                \"name\": section[\"name\"],\n",
        "                \"size\": section[\"size\"],\n",
        "                \"vsize\": section[\"vsize\"],\n",
        "                \"entropy\": section[\"entropy\"]\n",
        "            })\n",
        "\n",
        "            create_relationship(datapoint_node, \"HAS_SECTION\", section_node)\n",
        "\n",
        "            for prop in section[\"props\"]:\n",
        "                prop_node = create_node_if_not_exists(\"Prop\", {\"name\": prop})\n",
        "\n",
        "                create_relationship(section_node, \"HAS_PROP\", prop_node)\n",
        "        try:\n",
        "            imports = json.loads(row[41].replace(\"'\", '\"'))\n",
        "        except:\n",
        "            imports = {}\n",
        "        for key in imports:\n",
        "            list_of_imps = imports[key]\n",
        "            for li in list_of_imps:\n",
        "                imp = create_node_if_not_exists(\"Import\", {\"name\": key+'-'+li})\n",
        "                create_relationship(datapoint_node, \"HAS_IMPORT\", imp)\n",
        "        try:\n",
        "            exports = literal_eval(row[42])\n",
        "        except:\n",
        "            exports = []\n",
        "        for e in exports:\n",
        "            exp = create_node_if_not_exists(\"Export\", {\"name\": e})\n",
        "            create_relationship(datapoint_node, \"HAS_EXPORT\", exp)\n",
        "        try:\n",
        "            datadirectories = literal_eval(row[43])\n",
        "        except:\n",
        "            datadirectories = []\n",
        "        for t in datadirectories:\n",
        "            data_dir = create_node_if_not_exists(\"Directory\", {\"name\": t['name']})\n",
        "            create_relationship(datapoint_node, \"HAS_DIRECTORY\", data_dir)\n"
      ],
      "metadata": {
        "id": "-u3JWwP07D7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_node2vec():\n",
        "    tx = graph.begin()\n",
        "    tx.run(\"\"\"\n",
        "        CALL gds.graph.create(\n",
        "            'mitreember',\n",
        "            'Datapoint'\n",
        "        )\n",
        "        \"\"\")\n",
        "    tx.run(\"\"\"\n",
        "        CALL gds.alpha.node2vec.write('mitreember', {\n",
        "            embeddingDimension: 64,\n",
        "            walkLength: 10,\n",
        "            walksPerNode: 10,\n",
        "            returnFactor: 1.0,\n",
        "            inOutFactor: 1.0,\n",
        "            writeProperty: 'embedding'\n",
        "        })\n",
        "        \"\"\")\n",
        "    tx.commit()"
      ],
      "metadata": {
        "id": "QxV-KWWl6T4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_node_embeddings():\n",
        "    embs = []\n",
        "    results = graph.run(\"\"\"\n",
        "        CALL gds.beta.node2vec.stream(\"abc2\", {\n",
        "            relationshipWeightProperty: null,\n",
        "            iterations: 1,\n",
        "            embeddingDimension: 10,\n",
        "            walkLength: 80,\n",
        "            inOutFactor: 1,\n",
        "            returnFactor: 1\n",
        "        })\n",
        "        YIELD nodeId, embedding\n",
        "        RETURN gds.util.asNode(nodeId) AS node, embedding;\n",
        "        \"\"\")\n",
        "\n",
        "    for i, r in enumerate(results):\n",
        "        print(r['node']['index'], r[\"embedding\"])\n",
        "        embs.append(r['embedding'])\n",
        "\n",
        "    return embs"
      ],
      "metadata": {
        "id": "-u5N8vuJ6WXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embs = get_node_embeddings()\n",
        "df['Node Embeddings New Links'] = embs\n",
        "df.to_csv(DATASET_CSV)"
      ],
      "metadata": {
        "id": "ZHWcP6-r6bHO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}