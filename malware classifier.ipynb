{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c586fe-815d-4d70-8475-2675535e7c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in ./venv/lib/python3.9/site-packages (4.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./venv/lib/python3.9/site-packages (from jsonlines) (23.1.0)\n",
      "Requirement already satisfied: jsonpath_ng in ./venv/lib/python3.9/site-packages (1.6.0)\n",
      "Requirement already satisfied: ply in ./venv/lib/python3.9/site-packages (from jsonpath_ng) (3.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines\n",
    "!pip install jsonpath_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d826b9e-fee3-4f90-959f-8036018968f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from jsonpath_ng import parse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39749410-c6ce-492f-9ac5-caa2c80287c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_columns = ['sha256', 'md5', 'histogram', 'byteentropy', 'numstrings', 'avlength',\n",
    "               'printabledist', 'printables', 'string_entropy', 'paths', 'urls',\n",
    "               'registry', 'MZ', 'size', 'vsize', 'has_debug', 'exports', 'imports', 'has_relocations',\n",
    "               'has_resources', 'has_signature', 'has_tls', 'symbols',\n",
    "               'header_coff_machine', 'header_coff_characteristics', 'header_optional_subsystem',\n",
    "               'header_optional_dll_characteristics', 'header_optional_magic', 'header_optional_major_image_version',\n",
    "               'header_optional_minor_image_version', 'header_optional_major_linker_version',\n",
    "               'header_optional_minor_linker_version', 'header_optional_major_operating_system_version',\n",
    "               'header_optional_minor_operating_system_version', 'header_optional_major_subsystem_version',\n",
    "               'header_optional_minor_subsystem_version', 'header_optional_sizeof_code',\n",
    "               'header_optional_sizeof_headers', 'header_optional_sizeof_heap_commit', 'section', 'imports',\n",
    "                    'exports', 'datadirectories', 'label', 'avclass']\n",
    "\n",
    "jsonpath_columns = ['sha256', 'md5', 'histogram', 'byteentropy', 'strings/numstrings',\n",
    "                    'strings/avlength','strings/printabledist', 'strings/printables', 'strings/entropy',\n",
    "                    'strings/paths', 'strings/urls',\n",
    "               'strings/registry', 'strings/MZ', 'general/size', 'general/vsize', 'general/has_debug',\n",
    "                    'general/exports', 'general/imports', 'general/has_relocations',\n",
    "               'general/has_resources', 'general/has_signature', 'general/has_tls', 'general/symbols',\n",
    "               'header/coff/machine', 'header/coff/characteristics', 'header/optional/subsystem',\n",
    "               'header/optional/dll_characteristics', 'header/optional/magic', 'header/optional/major_image_version',\n",
    "               'header/optional/minor_image_version', 'header/optional/major_linker_version',\n",
    "               'header/optional/minor_linker_version', 'header/optional/major_operating_system_version',\n",
    "               'header/optional/minor_operating_system_version', 'header/optional/major_subsystem_version',\n",
    "               'header/optional/minor_subsystem_version', 'header/optional/sizeof_code',\n",
    "               'header/optional/sizeof_headers', 'header/optional/sizeof_heap_commit', 'section', 'imports',\n",
    "                    'exports', 'datadirectories', 'label', 'avclass']\n",
    "print(len(jsonpath_columns))\n",
    "print(len(pandas_columns))\n",
    "non_numerical_columns = ['sha256', 'md5', 'histogram', 'header_coff_machine', 'header_coff_characteristics',\n",
    "                         'header_optional_subsystem', 'printabledist', 'byteentropy',\n",
    "                         'header_optional_dll_characteristics', 'header_optional_magic']\n",
    "\n",
    "DATASET_CSV = 'dataset4.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d30066-9569-4029-9efb-a4ab347249d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jsonpath_value(json_data, col):\n",
    "    col = '.'.join(col.split('/'))\n",
    "    expr = parse(col)\n",
    "    matches = expr.find(json_data)\n",
    "    return matches[0].value if matches else None\n",
    "\n",
    "\n",
    "def write_row_into_csv(filename, row):\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "\n",
    "def process_line(line):\n",
    "    row = []\n",
    "    for col in jsonpath_columns:\n",
    "        if '/' in col:\n",
    "            row.append(get_jsonpath_value(line, col))\n",
    "        else:\n",
    "            row.append(line[col])\n",
    "    write_row_into_csv(DATASET_CSV, row)\n",
    "\n",
    "\n",
    "def prepare_dataset_parallel(filename):\n",
    "    with jsonlines.open('datasets/ember2018/' + filename) as f:\n",
    "        lines = list(f.iter())\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=40) as executor:  # Adjust the max_workers parameter as needed\n",
    "        futures = []\n",
    "        for line in tqdm(lines):\n",
    "            if line['label'] != -1:\n",
    "                future = executor.submit(process_line, line)\n",
    "                futures.append(future)\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def add_new_columns():\n",
    "    df = pd.read_csv(DATASET_CSV).drop(columns=['Unnamed: 0'])\n",
    "    files = ['train_features_1.jsonl']\n",
    "    datadirectories = []\n",
    "    for filename in files:\n",
    "        with jsonlines.open('datasets/ember2018/' + filename) as f:\n",
    "            lines = list(f.iter())\n",
    "        for line in tqdm(lines):\n",
    "            if line['label'] != -1:\n",
    "                datadirectories.append(line['datadirectories'])\n",
    "    df['datadirectories'] = datadirectories\n",
    "    df.to_csv(DATASET_CSV)\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac00a4-1e24-4d5e-9912-a5f00ceaf345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py2neo\n",
    "!pip install neo4j\n",
    "!pip install lightgbm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d3675-e936-45bf-a382-5a6b9bd45c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "\n",
    "cutoff = 18442\n",
    "df = pd.read_csv(DATASET_CSV, encoding='utf8', nrows=cutoff)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79625dd1-acce-4e98-9ee4-2d9293954b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_label(txt):\n",
    "\n",
    "\tif txt.startswith('intrusion-set'):\n",
    "\t\treturn 'Group'\n",
    "\tif txt.startswith('malware'):\n",
    "\t\treturn 'Software'\n",
    "\tif txt.startswith('tool'):\n",
    "\t\treturn 'Tool'\n",
    "\tif txt.startswith('attack-pattern'):\n",
    "\t\treturn 'Technique'\n",
    "\tif txt.startswith('course-of-action'):\n",
    "\t\treturn 'Technique'\n",
    "\treturn 'Unknown'\n",
    "\n",
    "\n",
    "def build_objects(obj,key):\n",
    "\tlabel = build_label(obj['type'])\n",
    "\tprops = {'name': obj['name'], 'id': obj['id'], 'type': obj['type']}\n",
    "\tif obj.get('description'):\t\tprops['description'] = obj['description']\n",
    "\tnode_main = Node(label, **props)\n",
    "\tgraph.merge(node_main,label,'name')\n",
    "\tprint('%s: \"%s\"' % (label,obj['name']),end='')\n",
    "\n",
    "\n",
    "\tif obj.get('aliases'):\n",
    "\t\taliases = obj['aliases']\n",
    "\telif obj.get('x_mitre_aliases'):\n",
    "\t\taliases = obj['x_mitre_aliases']\n",
    "\telse:\n",
    "\t\taliases = None\n",
    "\tif aliases:\n",
    "\t\tfor alias in aliases:\n",
    "\t\t\tif alias != obj['name']:\n",
    "\t\t\t\tnode_alias = Node('Alias', name=alias, type=obj['type'])\n",
    "\t\t\t\trelation = Relationship.type('alias')\n",
    "\t\t\t\tgraph.merge(relation(node_main,node_alias),label,'name')\n",
    "\t\t\t\tprint(' -[alias]-> \"%s\"' % (alias),end='')\n",
    "\n",
    "\n",
    "\n",
    "def build_relations(obj):\n",
    "\n",
    "\tif not gnames.get(obj['source_ref']): return\n",
    "\tif not gnames.get(obj['target_ref']): return\n",
    "\n",
    "\tm = NodeMatcher(graph)\n",
    "\n",
    "\tsource = m.match( build_label(obj['source_ref']), name=gnames[obj['source_ref']] ).first()\n",
    "\ttarget = m.match( build_label(obj['target_ref']), name=gnames[obj['target_ref']] ).first()\n",
    "\n",
    "\n",
    "\trelation = Relationship.type( obj['relationship_type'] )\n",
    "\n",
    "\tgraph.merge(relation(source,target),build_label(obj['source_ref']),'name')\n",
    "\tprint('Relation: \"%s\" -[%s]-> \"%s\"' % (gnames[obj['source_ref']],obj['relationship_type'],gnames[obj['target_ref']]) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5fd72-b082-4d19-b7e3-d8e95829dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from py2neo import Graph, Node, Relationship, NodeMatcher, cypher\n",
    "\n",
    "\n",
    "json_file = 'enterprise-attack.json'\n",
    "\n",
    "try:\n",
    "\twith open(json_file) as fh:\n",
    "\t\tdata = json.load(fh)\n",
    "\tfh.close()\n",
    "except Exception as e:\n",
    "\tsys.stderr.write( '[ERROR] reading configuration file %s\\n' % json_file )\n",
    "\tsys.stderr.write( '[ERROR] %s\\n' % str(e) )\n",
    "\tsys.exit(1)\n",
    "\n",
    "gnames = {}\n",
    "\n",
    "\n",
    "for obj in data['objects']:\n",
    "\n",
    "\n",
    "\tif obj['type'] =='intrusion-set':\n",
    "\t\tgnames[obj['id']] = obj['name']\n",
    "\t\tbuild_objects(obj,'intrusion-set')\n",
    "\n",
    "\n",
    "\tif obj['type']=='malware':\n",
    "\t\tgnames[ obj['id'] ] = obj['name']\n",
    "\t\tbuild_objects(obj,'malware')\n",
    "\n",
    "\n",
    "\tif obj['type']=='tool':\n",
    "\t\tgnames[ obj['id'] ] = obj['name']\n",
    "\t\tbuild_objects(obj,'tool')\n",
    "\n",
    "\n",
    "\tif obj['type']=='attack-pattern':\n",
    "\t\tgnames[ obj['id'] ] = obj['name']\n",
    "\t\tbuild_objects(obj,'attack-pattern')\n",
    "\n",
    "  if obj['type']=='course-of-action':\n",
    "    gnames[ obj['id'] ] = obj['name']\n",
    "    build_objects(obj,'course-of-action')\n",
    "\n",
    "for obj in data['objects']:\n",
    "\tif obj['type'] =='relationship':\n",
    "\t\tbuild_relations(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f97719-2bac-43a5-bc5a-edc7dbab6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_node_if_not_exists(label, properties):\n",
    "    # Check if a node with the same properties exists\n",
    "    existing_node = graph.nodes.match(label, **properties).first()\n",
    "\n",
    "    if existing_node:\n",
    "        return existing_node\n",
    "    else:\n",
    "        new_node = Node(label, **properties)\n",
    "        graph.create(new_node)\n",
    "        return new_node\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_relationship(start_node, relationship_type, end_node):\n",
    "    # Check if a relationship already exists\n",
    "    existing_rel = graph.match_one(nodes=[start_node, end_node], r_type=relationship_type)\n",
    "\n",
    "    if not existing_rel:\n",
    "        graph.create(Relationship(start_node, relationship_type, end_node))\n",
    "\n",
    "\n",
    "\n",
    "header = False\n",
    "with open('dataset4.csv', newline='', encoding='utf8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in tqdm(reader):\n",
    "        if not header:\n",
    "            header = True\n",
    "            continue\n",
    "        try:\n",
    "            section = json.loads(row[40].replace(\"'\", '\"'))\n",
    "        except:\n",
    "            section = {\"entry\": \"UNK\", \"sections\": []}\n",
    "        datapoint_node = create_node_if_not_exists(\"Datapoint\", {\"index\": row[0],\n",
    "                                                                 \"sha256\": row[1]})\n",
    "        entry = create_node_if_not_exists(\"EntrySection\", {\"name\": section['entry']})\n",
    "        create_relationship(datapoint_node, \"HAS_ENTRY_SECTION\", entry)\n",
    "        for section in section[\"sections\"]:\n",
    "            section_node = create_node_if_not_exists(\"Section\", {\n",
    "                \"name\": section[\"name\"],\n",
    "                \"size\": section[\"size\"],\n",
    "                \"vsize\": section[\"vsize\"],\n",
    "                \"entropy\": section[\"entropy\"]\n",
    "            })\n",
    "\n",
    "            create_relationship(datapoint_node, \"HAS_SECTION\", section_node)\n",
    "\n",
    "            for prop in section[\"props\"]:\n",
    "                prop_node = create_node_if_not_exists(\"Prop\", {\"name\": prop})\n",
    "\n",
    "                create_relationship(section_node, \"HAS_PROP\", prop_node)\n",
    "        try:\n",
    "            imports = json.loads(row[41].replace(\"'\", '\"'))\n",
    "        except:\n",
    "            imports = {}\n",
    "        for key in imports:\n",
    "            list_of_imps = imports[key]\n",
    "            for li in list_of_imps:\n",
    "                imp = create_node_if_not_exists(\"Import\", {\"name\": key+'-'+li})\n",
    "                create_relationship(datapoint_node, \"HAS_IMPORT\", imp)\n",
    "        try:\n",
    "            exports = literal_eval(row[42])\n",
    "        except:\n",
    "            exports = []\n",
    "        for e in exports:\n",
    "            exp = create_node_if_not_exists(\"Export\", {\"name\": e})\n",
    "            create_relationship(datapoint_node, \"HAS_EXPORT\", exp)\n",
    "        try:\n",
    "            datadirectories = literal_eval(row[43])\n",
    "        except:\n",
    "            datadirectories = []\n",
    "        for t in datadirectories:\n",
    "            data_dir = create_node_if_not_exists(\"Directory\", {\"name\": t['name']})\n",
    "            create_relationship(datapoint_node, \"HAS_DIRECTORY\", data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80afb6ac-40b5-40c0-8c88-61b06ae48e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_node2vec():\n",
    "    tx = graph.begin()\n",
    "    tx.run(\"\"\"\n",
    "        CALL gds.graph.create(\n",
    "            'mitreember',\n",
    "            'Datapoint'\n",
    "        )\n",
    "        \"\"\")\n",
    "    tx.run(\"\"\"\n",
    "        CALL gds.alpha.node2vec.write('mitreember', {\n",
    "            embeddingDimension: 64,\n",
    "            walkLength: 10,\n",
    "            walksPerNode: 10,\n",
    "            returnFactor: 1.0,\n",
    "            inOutFactor: 1.0,\n",
    "            writeProperty: 'embedding'\n",
    "        })\n",
    "        \"\"\")\n",
    "    tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc55be5-c37d-44a7-a328-f39af4691931",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_node2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e8794-8664-4171-aa9f-0069399f52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_embeddings():\n",
    "    embs = []\n",
    "    results = graph.run(\"\"\"\n",
    "        CALL gds.beta.node2vec.stream(\"abc2\", {\n",
    "            relationshipWeightProperty: null,\n",
    "            iterations: 1,\n",
    "            embeddingDimension: 10,\n",
    "            walkLength: 80,\n",
    "            inOutFactor: 1,\n",
    "            returnFactor: 1\n",
    "        })\n",
    "        YIELD nodeId, embedding\n",
    "        RETURN gds.util.asNode(nodeId) AS node, embedding;\n",
    "        \"\"\")\n",
    "\n",
    "    for i, r in enumerate(results):\n",
    "        print(r['node']['index'], r[\"embedding\"])\n",
    "        embs.append(r['embedding'])\n",
    "\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d1950-ef9f-4b59-8e8c-9fc39d5c3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = get_node_embeddings()\n",
    "df['Node Embeddings'] = embs\n",
    "df.to_csv(DATASET_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcef601-e547-4066-a3f4-b10a0262009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "\n",
    "model_filename = 'lightgbm_model_node_embs.txt'\n",
    "cutoff = 18442\n",
    "df = pd.read_csv(DATASET_CSV, encoding='utf8', nrows=cutoff)\n",
    "df = df[df['label'].isin([0, 1])]\n",
    "value_counts = df['label'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(value_counts)\n",
    "X_features = ['numstrings', 'avlength', 'printables', 'string_entropy', 'paths', 'urls', 'registry', 'MZ',\n",
    "             'size', 'vsize', 'has_debug', 'exports', 'imports', 'has_relocations', 'has_signature', 'has_tls', 'symbols',\n",
    "             'header_optional_major_image_version', 'header_optional_minor_image_version', 'header_optional_major_linker_version',\n",
    "             'header_optional_minor_linker_version', 'header_optional_major_operating_system_version', 'header_optional_minor_operating_system_version',\n",
    "             'header_optional_major_subsystem_version', 'header_optional_minor_subsystem_version', 'header_optional_sizeof_code', 'header_optional_sizeof_headers',\n",
    "             'header_optional_sizeof_heap_commit', 'histogram_entropy', 'printabledist_entropy', 'byteentropy_entropy', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "\n",
    "def convert_to_list(embeddings_string):\n",
    "    embeddings = np.array(literal_eval(embeddings_string))\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['Node Embeddings'] = df['Node Embeddings'].progress_apply(convert_to_list)\n",
    "X_flattened = df['Node Embeddings'].apply(pd.Series)\n",
    "X_flattened = pd.concat([df, X_flattened], axis=1)\n",
    "print(X_flattened.columns)\n",
    "X = X_flattened.drop(columns=['Node Embeddings'])\n",
    "pickle_file_path = 'dataset5_with_embs.pkl'\n",
    "X.to_pickle(pickle_file_path)\n",
    "X = X[X_features].values\n",
    "y = df['label'].values\n",
    "\n",
    "print(\"Length:\")\n",
    "print(len(X), len(y))\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 300,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.6,\n",
    "    'data_sample_strategy': 'bagging'\n",
    "}\n",
    "\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "bst.save_model(model_filename)\n",
    "\n",
    "bst = lgb.Booster(model_file=model_filename)\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "f1_sc = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 score: {f1_sc}')\n",
    "\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_binary)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e347223-2a19-4d33-af3b-dc73d5838a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "shap_values = shap.TreeExplainer(bst).shap_values(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-env",
   "language": "python",
   "name": "python-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
