{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c586fe-815d-4d70-8475-2675535e7c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonlines) (23.1.0)\n",
      "Requirement already satisfied: jsonpath_ng in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: ply in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonpath_ng) (3.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines\n",
    "!pip install jsonpath_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d826b9e-fee3-4f90-959f-8036018968f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from jsonpath_ng import parse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39749410-c6ce-492f-9ac5-caa2c80287c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "pandas_columns = ['sha256', 'md5', 'histogram', 'byteentropy', 'numstrings', 'avlength',\n",
    "               'printabledist', 'printables', 'string_entropy', 'paths', 'urls',\n",
    "               'registry', 'MZ', 'size', 'vsize', 'has_debug', 'exports', 'imports', 'has_relocations',\n",
    "               'has_resources', 'has_signature', 'has_tls', 'symbols',\n",
    "               'header_coff_machine', 'header_coff_characteristics', 'header_optional_subsystem',\n",
    "               'header_optional_dll_characteristics', 'header_optional_magic', 'header_optional_major_image_version',\n",
    "               'header_optional_minor_image_version', 'header_optional_major_linker_version',\n",
    "               'header_optional_minor_linker_version', 'header_optional_major_operating_system_version',\n",
    "               'header_optional_minor_operating_system_version', 'header_optional_major_subsystem_version',\n",
    "               'header_optional_minor_subsystem_version', 'header_optional_sizeof_code',\n",
    "               'header_optional_sizeof_headers', 'header_optional_sizeof_heap_commit', 'section', 'imports',\n",
    "                    'exports', 'datadirectories', 'label', 'avclass']\n",
    "\n",
    "jsonpath_columns = ['sha256', 'md5', 'histogram', 'byteentropy', 'strings/numstrings',\n",
    "                    'strings/avlength','strings/printabledist', 'strings/printables', 'strings/entropy',\n",
    "                    'strings/paths', 'strings/urls',\n",
    "               'strings/registry', 'strings/MZ', 'general/size', 'general/vsize', 'general/has_debug',\n",
    "                    'general/exports', 'general/imports', 'general/has_relocations',\n",
    "               'general/has_resources', 'general/has_signature', 'general/has_tls', 'general/symbols',\n",
    "               'header/coff/machine', 'header/coff/characteristics', 'header/optional/subsystem',\n",
    "               'header/optional/dll_characteristics', 'header/optional/magic', 'header/optional/major_image_version',\n",
    "               'header/optional/minor_image_version', 'header/optional/major_linker_version',\n",
    "               'header/optional/minor_linker_version', 'header/optional/major_operating_system_version',\n",
    "               'header/optional/minor_operating_system_version', 'header/optional/major_subsystem_version',\n",
    "               'header/optional/minor_subsystem_version', 'header/optional/sizeof_code',\n",
    "               'header/optional/sizeof_headers', 'header/optional/sizeof_heap_commit', 'section', 'imports',\n",
    "                    'exports', 'datadirectories', 'label', 'avclass']\n",
    "print(len(jsonpath_columns))\n",
    "print(len(pandas_columns))\n",
    "non_numerical_columns = ['sha256', 'md5', 'histogram', 'header_coff_machine', 'header_coff_characteristics',\n",
    "                         'header_optional_subsystem', 'printabledist', 'byteentropy',\n",
    "                         'header_optional_dll_characteristics', 'header_optional_magic']\n",
    "\n",
    "DATASET_CSV = 'datasets/dataset4.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d30066-9569-4029-9efb-a4ab347249d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jsonpath_value(json_data, col):\n",
    "    col = '.'.join(col.split('/'))\n",
    "    expr = parse(col)\n",
    "    matches = expr.find(json_data)\n",
    "    return matches[0].value if matches else None\n",
    "\n",
    "\n",
    "def write_row_into_csv(filename, row):\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "\n",
    "def process_line(line):\n",
    "    row = []\n",
    "    for col in jsonpath_columns:\n",
    "        if '/' in col:\n",
    "            row.append(get_jsonpath_value(line, col))\n",
    "        else:\n",
    "            row.append(line[col])\n",
    "    write_row_into_csv(DATASET_CSV, row)\n",
    "\n",
    "\n",
    "def prepare_dataset_parallel(filename):\n",
    "    with jsonlines.open('datasets/' + filename) as f:\n",
    "        lines = list(f.iter())\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=40) as executor:  # Adjust the max_workers parameter as needed\n",
    "        futures = []\n",
    "        for line in tqdm(lines):\n",
    "            if line['label'] != -1:\n",
    "                future = executor.submit(process_line, line)\n",
    "                futures.append(future)\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def add_new_columns():\n",
    "    df = pd.read_csv(DATASET_CSV).drop(columns=['Unnamed: 0'])\n",
    "    files = ['train_features_1.jsonl']\n",
    "    datadirectories = []\n",
    "    for filename in files:\n",
    "        with jsonlines.open('datasets/' + filename) as f:\n",
    "            lines = list(f.iter())\n",
    "        for line in tqdm(lines):\n",
    "            if line['label'] != -1:\n",
    "                datadirectories.append(line['datadirectories'])\n",
    "    df['datadirectories'] = datadirectories\n",
    "    df.to_csv(DATASET_CSV)\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c8fbccd-0bfb-4d7c-9b14-72ee5e80294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(DATASET_CSV).drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5efca5a7-b3cf-44d5-8126-f3de46154737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(DATASET_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac00a4-1e24-4d5e-9912-a5f00ceaf345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py2neo\n",
    "!pip install neo4j\n",
    "!pip install lightgbm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251931ee-7ae2-4eb2-8018-4932d7754d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset_parallel('train_features_1.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "296d3675-e936-45bf-a382-5a6b9bd45c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sha256  \\\n",
      "0  b77209803574971357b84466225a463ca9fe429c9a1444...   \n",
      "1  50d63e9d80cb8838bad3338e8ff370897e48d5a24dbc86...   \n",
      "2  0af5e51c258aebd88076800690a9432b63a578b459e669...   \n",
      "3  ce84c2b0a5589899896395875677fbd301262847457ecc...   \n",
      "4  b61064dd5d6eb23ea1681b9bf90f050c939725bcf09dc8...   \n",
      "\n",
      "                                md5  \\\n",
      "0  ecb2e2191141fe4c5bf844033f717886   \n",
      "1  6684314acd5c92511cc2cb3119193d1b   \n",
      "2  635f280263eb12a11c3eaf10900a29b3   \n",
      "3  202d58b248c44178e5adf44926662438   \n",
      "4  a22ccc13451d9fbe20ff00abb4df7659   \n",
      "\n",
      "                                           histogram  \\\n",
      "0  [21205, 705, 366, 261, 360, 234, 222, 243, 553...   \n",
      "1  [43265, 6128, 5031, 5140, 5772, 4256, 4277, 41...   \n",
      "2  [155338, 7032, 4852, 4281, 5913, 3495, 3975, 4...   \n",
      "3  [17340, 404, 1195, 178, 440, 1087, 116, 123, 7...   \n",
      "4  [86175, 5213, 3014, 3157, 4285, 2115, 1447, 29...   \n",
      "\n",
      "                                         byteentropy  numstrings   avlength  \\\n",
      "0  [2048, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         318  15.069182   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        4771   6.944666   \n",
      "2  [75776, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...        4042   7.741217   \n",
      "3  [2048, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         121  27.570248   \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        3746  10.090763   \n",
      "\n",
      "                                       printabledist  printables  \\\n",
      "0  [192, 11, 32, 11, 17, 11, 6, 44, 19, 9, 11, 10...        4792   \n",
      "1  [811, 249, 301, 245, 265, 260, 248, 323, 228, ...       33133   \n",
      "2  [716, 166, 184, 166, 263, 172, 195, 195, 222, ...       31290   \n",
      "3  [55, 2, 2, 0, 1, 12, 1, 0, 11, 3, 2, 2, 6, 15,...        3336   \n",
      "4  [214, 58, 182, 77, 246, 55, 69, 53, 144, 48, 8...       37800   \n",
      "\n",
      "   string_entropy  paths  ...  header_optional_sizeof_heap_commit  \\\n",
      "0        5.813516      0  ...                                4096   \n",
      "1        6.464035      0  ...                                4096   \n",
      "2        6.404451      0  ...                                4096   \n",
      "3        5.694308      0  ...                                4096   \n",
      "4        6.091294      0  ...                                4096   \n",
      "\n",
      "                                             section  \\\n",
      "0  {'entry': '.text', 'sections': [{'name': '.tex...   \n",
      "1  {'entry': '.text', 'sections': [{'name': '.tex...   \n",
      "2  {'entry': '.text', 'sections': [{'name': '.tex...   \n",
      "3  {'entry': '.text', 'sections': [{'name': '.tex...   \n",
      "4  {'entry': 'CODE', 'sections': [{'name': 'CODE'...   \n",
      "\n",
      "                                           imports.1  exports.1  \\\n",
      "0  {'USER32.dll': ['EndDialog', 'SetWindowTextW',...         []   \n",
      "1  {'KERNEL32.dll': ['GetLastError', 'SetLastErro...         []   \n",
      "2  {'USER32.dll': ['GetWindowRect', 'GetWindow', ...         []   \n",
      "3  {'CRYPT32.dll': ['CryptBinaryToStringW', 'Cryp...         []   \n",
      "4  {'kernel32.dll': ['DeleteCriticalSection', 'Tl...         []   \n",
      "\n",
      "                                     datadirectories  label   avclass  \\\n",
      "0  [{'name': 'EXPORT_TABLE', 'size': 0, 'virtual_...      1    upatre   \n",
      "1  [{'name': 'EXPORT_TABLE', 'size': 51, 'virtual...      0       NaN   \n",
      "2  [{'name': 'EXPORT_TABLE', 'size': 0, 'virtual_...      1  adposhel   \n",
      "3  [{'name': 'EXPORT_TABLE', 'size': 0, 'virtual_...      1   kasidet   \n",
      "4  [{'name': 'EXPORT_TABLE', 'size': 0, 'virtual_...      1   dealply   \n",
      "\n",
      "   histogram_entropy  printabledist_entropy  byteentropy_entropy  \n",
      "0           6.018028               5.813516             6.296935  \n",
      "1           7.879034               6.464035             5.051957  \n",
      "2           5.033061               6.404452             4.478684  \n",
      "3           5.833126               5.694308             5.361433  \n",
      "4           6.883026               6.091295             5.370638  \n",
      "\n",
      "[5 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "\n",
    "cutoff = 18442\n",
    "df = pd.read_csv(DATASET_CSV, encoding='utf8', nrows=cutoff)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94cb8d71-ae5c-46e2-a7d3-9194f63f1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"bolt://localhost:7687\", name='mitreember', auth=(\"neo4j\", \"12345678\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79625dd1-acce-4e98-9ee4-2d9293954b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_label(txt):\n",
    "\n",
    "\tif txt.startswith('intrusion-set'):\n",
    "\t\treturn 'Group'\n",
    "\tif txt.startswith('malware'):\n",
    "\t\treturn 'Software'\n",
    "\tif txt.startswith('tool'):\n",
    "\t\treturn 'Tool'\n",
    "\tif txt.startswith('attack-pattern'):\n",
    "\t\treturn 'Technique'\n",
    "\tif txt.startswith('course-of-action'):\n",
    "\t\treturn 'Technique'\n",
    "\treturn 'Unknown'\n",
    "\n",
    "\n",
    "def build_objects(obj,key):\n",
    "\tlabel = build_label(obj['type'])\n",
    "\tprops = {'name': obj['name'], 'id': obj['id'], 'type': obj['type']}\n",
    "\tif obj.get('description'):\t\tprops['description'] = obj['description']\n",
    "\tnode_main = Node(label, **props)\n",
    "\tgraph.merge(node_main,label,'name')\n",
    "\t#print('%s: \"%s\"' % (label,obj['name']),end='')\n",
    "\n",
    "\n",
    "\tif obj.get('aliases'):\n",
    "\t\taliases = obj['aliases']\n",
    "\telif obj.get('x_mitre_aliases'):\n",
    "\t\taliases = obj['x_mitre_aliases']\n",
    "\telse:\n",
    "\t\taliases = None\n",
    "\tif aliases:\n",
    "\t\tfor alias in aliases:\n",
    "\t\t\tif alias != obj['name']:\n",
    "\t\t\t\tnode_alias = Node('Alias', name=alias, type=obj['type'])\n",
    "\t\t\t\trelation = Relationship.type('alias')\n",
    "\t\t\t\tgraph.merge(relation(node_main,node_alias),label,'name')\n",
    "\t\t\t\t#print(' -[alias]-> \"%s\"' % (alias),end='')\n",
    "\n",
    "\n",
    "\n",
    "def build_relations(obj):\n",
    "\n",
    "\tif not gnames.get(obj['source_ref']): return\n",
    "\tif not gnames.get(obj['target_ref']): return\n",
    "\n",
    "\tm = NodeMatcher(graph)\n",
    "\n",
    "\tsource = m.match( build_label(obj['source_ref']), name=gnames[obj['source_ref']] ).first()\n",
    "\ttarget = m.match( build_label(obj['target_ref']), name=gnames[obj['target_ref']] ).first()\n",
    "\n",
    "\n",
    "\trelation = Relationship.type( obj['relationship_type'] )\n",
    "\n",
    "\tgraph.merge(relation(source,target),build_label(obj['source_ref']),'name')\n",
    "\t#print('Relation: \"%s\" -[%s]-> \"%s\"' % (gnames[obj['source_ref']],obj['relationship_type'],gnames[obj['target_ref']]) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bb5fd72-b082-4d19-b7e3-d8e95829dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from py2neo import Graph, Node, Relationship, NodeMatcher, cypher\n",
    "\n",
    "\n",
    "json_file = 'datasets/enterprise-attack.json'\n",
    "\n",
    "try:\n",
    "\twith open(json_file) as fh:\n",
    "\t\tdata = json.load(fh)\n",
    "\tfh.close()\n",
    "except Exception as e:\n",
    "\tsys.stderr.write( '[ERROR] reading configuration file %s\\n' % json_file )\n",
    "\tsys.stderr.write( '[ERROR] %s\\n' % str(e) )\n",
    "\tsys.exit(1)\n",
    "\n",
    "gnames = {}\n",
    "\n",
    "\n",
    "for obj in data['objects']:\n",
    "\n",
    "\n",
    "\tif obj['type'] =='intrusion-set':\n",
    "\t\tgnames[obj['id']] = obj['name']\n",
    "\t\tbuild_objects(obj,'intrusion-set')\n",
    "\n",
    "\n",
    "\tif obj['type']=='malware':\n",
    "\t\tgnames[ obj['id'] ] = obj['name']\n",
    "\t\tbuild_objects(obj,'malware')\n",
    "\n",
    "\n",
    "\tif obj['type']=='tool':\n",
    "\t\tgnames[ obj['id'] ] = obj['name']\n",
    "\t\tbuild_objects(obj,'tool')\n",
    "\n",
    "\n",
    "\tif obj['type']=='attack-pattern':\n",
    "\t\tgnames[ obj['id'] ] = obj['name']\n",
    "\t\tbuild_objects(obj,'attack-pattern')\n",
    "        \n",
    "\tif obj['type']=='course-of-action':\n",
    "\t\tgnames[ obj['id'] ] = obj['name']\n",
    "\t\tbuild_objects(obj,'course-of-action')\n",
    "\n",
    "for obj in data['objects']:\n",
    "\tif obj['type'] =='relationship':\n",
    "\t\tbuild_relations(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31f97719-2bac-43a5-bc5a-edc7dbab6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_node_if_not_exists(label, properties):\n",
    "    # Check if a node with the same properties exists\n",
    "    existing_node = graph.nodes.match(label, **properties).first()\n",
    "\n",
    "    if existing_node:\n",
    "        return existing_node\n",
    "    else:\n",
    "        new_node = Node(label, **properties)\n",
    "        graph.create(new_node)\n",
    "        return new_node\n",
    "\n",
    "\n",
    "def create_relationship(start_node, relationship_type, end_node):\n",
    "    # Check if a relationship already exists\n",
    "    existing_rel = graph.match_one(nodes=[start_node, end_node], r_type=relationship_type)\n",
    "\n",
    "    if not existing_rel:\n",
    "        graph.create(Relationship(start_node, relationship_type, end_node))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8ba4f-f1d5-43dd-93dc-4e73a1040de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = False\n",
    "with open(DATASET_CSV, newline='', encoding='utf8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in tqdm(reader):\n",
    "        if not header:\n",
    "            header = True\n",
    "            print(row)\n",
    "            continue\n",
    "        try:\n",
    "            section = json.loads(row[39].replace(\"'\", '\"'))\n",
    "        except:\n",
    "            section = {\"entry\": \"UNK\", \"sections\": []}\n",
    "        datapoint_node = create_node_if_not_exists(\"Datapoint\", {\"index\": row[0],\n",
    "                                                                 \"sha256\": row[1]})\n",
    "        entry = create_node_if_not_exists(\"EntrySection\", {\"name\": section['entry']})\n",
    "        create_relationship(datapoint_node, \"HAS_ENTRY_SECTION\", entry)\n",
    "        for section in section[\"sections\"]:\n",
    "            section_node = create_node_if_not_exists(\"Section\", {\n",
    "                \"name\": section[\"name\"],\n",
    "                \"size\": section[\"size\"],\n",
    "                \"vsize\": section[\"vsize\"],\n",
    "                \"entropy\": section[\"entropy\"]\n",
    "            })\n",
    "\n",
    "            create_relationship(datapoint_node, \"HAS_SECTION\", section_node)\n",
    "\n",
    "            for prop in section[\"props\"]:\n",
    "                prop_node = create_node_if_not_exists(\"Prop\", {\"name\": prop})\n",
    "\n",
    "                create_relationship(section_node, \"HAS_PROP\", prop_node)\n",
    "        try:\n",
    "            imports = json.loads(row[40].replace(\"'\", '\"'))\n",
    "        except:\n",
    "            imports = {}\n",
    "        for key in imports:\n",
    "            list_of_imps = imports[key]\n",
    "            for li in list_of_imps:\n",
    "                imp = create_node_if_not_exists(\"Import\", {\"name\": key+'-'+li})\n",
    "                create_relationship(datapoint_node, \"HAS_IMPORT\", imp)\n",
    "        try:\n",
    "            exports = literal_eval(row[41])\n",
    "        except:\n",
    "            exports = []\n",
    "        for e in exports:\n",
    "            exp = create_node_if_not_exists(\"Export\", {\"name\": e})\n",
    "            create_relationship(datapoint_node, \"HAS_EXPORT\", exp)\n",
    "        try:\n",
    "            datadirectories = literal_eval(row[42])\n",
    "        except:\n",
    "            datadirectories = []\n",
    "        for t in datadirectories:\n",
    "            data_dir = create_node_if_not_exists(\"Directory\", {\"name\": t['name']})\n",
    "            create_relationship(datapoint_node, \"HAS_DIRECTORY\", data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "80afb6ac-40b5-40c0-8c88-61b06ae48e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_node2vec():\n",
    "    tx = graph.begin()\n",
    "    \n",
    "    # Define parameters\n",
    "    limit = 42\n",
    "    config = {\n",
    "        'relationshipWeightProperty': None,\n",
    "        'iterations': 1,\n",
    "        'embeddingDimension': 10,\n",
    "        'walkLength': 80,\n",
    "        'inOutFactor': 1,\n",
    "        'returnFactor': 1\n",
    "    }\n",
    "    graphConfig = {\n",
    "        'nodeProjection': '*',\n",
    "        'relationshipProjection': {\n",
    "            'relType': {\n",
    "                'type': '*',\n",
    "                'orientation': 'UNDIRECTED',\n",
    "                'properties': {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    communityNodeLimit = 10\n",
    "    generatedName = 'abc2'\n",
    "\n",
    "    tx.run(\"\"\"\n",
    "        CALL gds.graph.project($generatedName, $graphConfig.nodeProjection, $graphConfig.relationshipProjection, {})\n",
    "        \"\"\", generatedName=generatedName, graphConfig=graphConfig)\n",
    "    \n",
    "    tx.run(\"\"\"\n",
    "        CALL gds.beta.node2vec.stream($generatedName, $config)\n",
    "        YIELD nodeId, embedding\n",
    "        RETURN gds.util.asNode(nodeId) AS node, embedding\n",
    "        LIMIT toInteger($limit)\n",
    "        \"\"\", generatedName=generatedName, config=config, limit=limit)\n",
    "    \n",
    "    tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc55be5-c37d-44a7-a328-f39af4691931",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_node2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b5e8794-8664-4171-aa9f-0069399f52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_embeddings():\n",
    "    embs = []\n",
    "    results = graph.run(\"\"\"\n",
    "        CALL gds.beta.node2vec.stream(\"abc2\", {\n",
    "            relationshipWeightProperty: null,\n",
    "            iterations: 1,\n",
    "            embeddingDimension: 10,\n",
    "            walkLength: 80,\n",
    "            inOutFactor: 1,\n",
    "            returnFactor: 1\n",
    "        })\n",
    "        YIELD nodeId, embedding\n",
    "        RETURN gds.util.asNode(nodeId) AS node, embedding;\n",
    "        \"\"\")\n",
    "\n",
    "    for i, r in enumerate(results):\n",
    "        print(r['node']['index'], r[\"embedding\"])\n",
    "        embs.append(r['embedding'])\n",
    "\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d1950-ef9f-4b59-8e8c-9fc39d5c3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = get_node_embeddings()\n",
    "df['Node Embeddings'] = embs\n",
    "df.to_csv(DATASET_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934361b-302b-4db5-a3fa-8ec337cc8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "list_columns = ['histogram', 'printabledist', 'byteentropy']\n",
    "df = pd.read_csv(DATASET_CSV, encoding='utf8')\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "def calculate_entropy(byte_entropy_values):\n",
    "    values_array = literal_eval(byte_entropy_values)\n",
    "    return entropy(values_array, base=2)\n",
    "\n",
    "for col in list_columns:\n",
    "    tqdm.pandas(desc=f'Calculating entropy for {col}')\n",
    "    df[col+'_entropy'] = df[col].progress_apply(calculate_entropy)\n",
    "\n",
    "df.to_csv(DATASET_CSV, encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcef601-e547-4066-a3f4-b10a0262009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "model_filename = 'models/lightgbm_model_node_embs.txt'\n",
    "cutoff = 18442\n",
    "df = pd.read_csv(DATASET_CSV, encoding='utf8', nrows=cutoff)\n",
    "df = df[df['label'].isin([0, 1])]\n",
    "value_counts = df['label'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(value_counts)\n",
    "X_features = ['numstrings', 'avlength', 'printables', 'string_entropy', 'paths', 'urls', 'registry', 'MZ',\n",
    "             'size', 'vsize', 'has_debug', 'exports', 'imports', 'has_relocations', 'has_signature', 'has_tls', 'symbols',\n",
    "             'header_optional_major_image_version', 'header_optional_minor_image_version', 'header_optional_major_linker_version',\n",
    "             'header_optional_minor_linker_version', 'header_optional_major_operating_system_version', 'header_optional_minor_operating_system_version',\n",
    "             'header_optional_major_subsystem_version', 'header_optional_minor_subsystem_version', 'header_optional_sizeof_code', 'header_optional_sizeof_headers',\n",
    "             'header_optional_sizeof_heap_commit', 'histogram_entropy', 'printabledist_entropy', 'byteentropy_entropy', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "\n",
    "def convert_to_list(embeddings_string):\n",
    "    embeddings = np.array(literal_eval(embeddings_string))\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "df['Node Embeddings'] = df['Node Embeddings'].progress_apply(convert_to_list)\n",
    "X_flattened = df['Node Embeddings'].apply(pd.Series)\n",
    "X_flattened = pd.concat([df, X_flattened], axis=1)\n",
    "print(X_flattened.columns)\n",
    "X = X_flattened.drop(columns=['Node Embeddings'])\n",
    "pickle_file_path = 'dataset5_with_embs.pkl'\n",
    "X.to_pickle(pickle_file_path)\n",
    "X = X[X_features].values\n",
    "y = df['label'].values\n",
    "\n",
    "print(\"Length:\")\n",
    "print(len(X), len(y))\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 300,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.6,\n",
    "    'data_sample_strategy': 'bagging'\n",
    "}\n",
    "\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "bst.save_model(model_filename)\n",
    "\n",
    "bst = lgb.Booster(model_file=model_filename)\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "f1_sc = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 score: {f1_sc}')\n",
    "\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_binary)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e347223-2a19-4d33-af3b-dc73d5838a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap_values = shap.TreeExplainer(bst).shap_values(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-env",
   "language": "python",
   "name": "python-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
